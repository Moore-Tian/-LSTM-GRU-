{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code uses a GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "from modle import *\n",
    "from utils import *\n",
    "from vocab import *\n",
    "\n",
    "file_path = \"dataset/poetryFromTang.txt\"\n",
    "\n",
    "char_list = get_dataset(file_path)\n",
    "\n",
    "vocab = Vocab(char_list)\n",
    "\n",
    "data_set = data_process(file_path, vocab)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"The code uses a GPU\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:  2516\n"
     ]
    }
   ],
   "source": [
    "vocab_size = vocab.vocab_size\n",
    "dim_emb = 100\n",
    "hidden_size = 500\n",
    "num_layers = 5\n",
    "limit = 100\n",
    "\n",
    "print(\"vocab_size: \", vocab_size)\n",
    "\n",
    "model = LSTM_LM(vocab_size, dim_emb, hidden_size, num_layers, limit, device).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 loss = 556.996582\n",
      "Epoch: 0020 loss = 471.674011\n",
      "Epoch: 0030 loss = 382.423462\n",
      "Epoch: 0040 loss = 531.330688\n",
      "Epoch: 0050 loss = 475.657349\n",
      "Epoch: 0060 loss = 821.005737\n",
      "Epoch: 0070 loss = 604.950195\n",
      "Epoch: 0080 loss = 360.292175\n",
      "Epoch: 0090 loss = 284.666351\n",
      "Epoch: 0100 loss = 531.362061\n",
      "Epoch: 0110 loss = 354.539673\n",
      "Epoch: 0120 loss = 674.445618\n",
      "Epoch: 0130 loss = 660.758545\n",
      "Epoch: 0140 loss = 309.999939\n",
      "Epoch: 0150 loss = 321.791077\n",
      "Epoch: 0160 loss = 360.221924\n",
      "Epoch: 0170 loss = 634.338257\n",
      "Epoch: 0180 loss = 326.306091\n",
      "Epoch: 0190 loss = 263.962830\n",
      "Epoch: 0200 loss = 455.301208\n",
      "Epoch: 0210 loss = 350.864594\n",
      "Epoch: 0220 loss = 322.670105\n",
      "Epoch: 0230 loss = 278.413757\n",
      "Epoch: 0240 loss = 270.538971\n",
      "Epoch: 0250 loss = 242.285065\n",
      "Epoch: 0260 loss = 197.960907\n",
      "Epoch: 0270 loss = 160.185883\n",
      "Epoch: 0280 loss = 465.396057\n",
      "Epoch: 0290 loss = 191.659210\n",
      "Epoch: 0300 loss = 320.083252\n",
      "Epoch: 0310 loss = 265.597168\n",
      "Epoch: 0320 loss = 470.952454\n",
      "Epoch: 0330 loss = 251.365936\n",
      "Epoch: 0340 loss = 375.552551\n",
      "Epoch: 0350 loss = 263.623474\n",
      "Epoch: 0360 loss = 225.721481\n",
      "Epoch: 0370 loss = 186.507996\n",
      "Epoch: 0380 loss = 148.509369\n",
      "Epoch: 0390 loss = 395.539368\n",
      "Epoch: 0400 loss = 97.527466\n",
      "Epoch: 0410 loss = 184.740952\n",
      "Epoch: 0420 loss = 273.101624\n",
      "Epoch: 0430 loss = 151.325134\n",
      "Epoch: 0440 loss = 92.617920\n",
      "Epoch: 0450 loss = 142.168930\n",
      "Epoch: 0460 loss = 220.839645\n",
      "Epoch: 0470 loss = 123.361664\n",
      "Epoch: 0480 loss = 109.573380\n",
      "Epoch: 0490 loss = 92.499146\n",
      "Epoch: 0500 loss = 147.486755\n",
      "Epoch: 0510 loss = 194.976135\n",
      "Epoch: 0520 loss = 96.803757\n",
      "Epoch: 0530 loss = 129.395752\n",
      "Epoch: 0540 loss = 88.141930\n",
      "Epoch: 0550 loss = 96.645851\n",
      "Epoch: 0560 loss = 193.332581\n",
      "Epoch: 0570 loss = 168.956146\n",
      "Epoch: 0580 loss = 66.238617\n",
      "Epoch: 0590 loss = 85.167091\n",
      "Epoch: 0600 loss = 94.477737\n",
      "Epoch: 0610 loss = 97.875885\n",
      "Epoch: 0620 loss = 258.335083\n",
      "Epoch: 0630 loss = 107.158447\n",
      "Epoch: 0640 loss = 52.120518\n",
      "Epoch: 0650 loss = 166.269104\n",
      "Epoch: 0660 loss = 248.716003\n",
      "Epoch: 0670 loss = 302.445374\n",
      "Epoch: 0680 loss = 117.275093\n",
      "Epoch: 0690 loss = 150.025757\n",
      "Epoch: 0700 loss = 80.132538\n",
      "Epoch: 0710 loss = 80.173065\n",
      "Epoch: 0720 loss = 82.882759\n",
      "Epoch: 0730 loss = 133.136322\n",
      "Epoch: 0740 loss = 65.694672\n",
      "Epoch: 0750 loss = 69.971268\n",
      "Epoch: 0760 loss = 31.681885\n",
      "Epoch: 0770 loss = 51.217796\n",
      "Epoch: 0780 loss = 43.128738\n",
      "Epoch: 0790 loss = 43.785000\n",
      "Epoch: 0800 loss = 29.604584\n",
      "Epoch: 0810 loss = 22.137383\n",
      "Epoch: 0820 loss = 73.694733\n",
      "Epoch: 0830 loss = 65.853714\n",
      "Epoch: 0840 loss = 11.560645\n",
      "Epoch: 0850 loss = 64.126038\n",
      "Epoch: 0860 loss = 23.742897\n",
      "Epoch: 0870 loss = 16.570473\n",
      "Epoch: 0880 loss = 15.998661\n",
      "Epoch: 0890 loss = 27.709806\n",
      "Epoch: 0900 loss = 9.286041\n",
      "Epoch: 0910 loss = 20.222137\n",
      "Epoch: 0920 loss = 29.616653\n",
      "Epoch: 0930 loss = 13.508196\n",
      "Epoch: 0940 loss = 24.788883\n",
      "Epoch: 0950 loss = 12.045709\n",
      "Epoch: 0960 loss = 10.145428\n",
      "Epoch: 0970 loss = 10.825171\n",
      "Epoch: 0980 loss = 14.752944\n",
      "Epoch: 0990 loss = 26.844114\n",
      "Epoch: 1000 loss = 7.617753\n",
      "Epoch: 1010 loss = 8.567428\n",
      "Epoch: 1020 loss = 3.984223\n",
      "Epoch: 1030 loss = 14.108034\n",
      "Epoch: 1040 loss = 14.621881\n",
      "Epoch: 1050 loss = 24.748138\n",
      "Epoch: 1060 loss = 14.524216\n",
      "Epoch: 1070 loss = 11.622261\n",
      "Epoch: 1080 loss = 10.344225\n",
      "Epoch: 1090 loss = 8.552008\n",
      "Epoch: 1100 loss = 8.955263\n",
      "Epoch: 1110 loss = 3.590133\n",
      "Epoch: 1120 loss = 6.648197\n",
      "Epoch: 1130 loss = 3.166213\n",
      "Epoch: 1140 loss = 5.116103\n",
      "Epoch: 1150 loss = 5.005873\n",
      "Epoch: 1160 loss = 4.252049\n",
      "Epoch: 1170 loss = 2.725134\n",
      "Epoch: 1180 loss = 7.400441\n",
      "Epoch: 1190 loss = 3.437845\n",
      "Epoch: 1200 loss = 2.839044\n",
      "Epoch: 1210 loss = 3.497457\n",
      "Epoch: 1220 loss = 5.777168\n",
      "Epoch: 1230 loss = 3.322494\n",
      "Epoch: 1240 loss = 5.751118\n",
      "Epoch: 1250 loss = 3.501609\n",
      "Epoch: 1260 loss = 2.051345\n",
      "Epoch: 1270 loss = 1.260230\n",
      "Epoch: 1280 loss = 6.894633\n",
      "Epoch: 1290 loss = 1.125163\n",
      "Epoch: 1300 loss = 1.479091\n",
      "Epoch: 1310 loss = 2.432349\n",
      "Epoch: 1320 loss = 4.869676\n",
      "Epoch: 1330 loss = 3.225035\n",
      "Epoch: 1340 loss = 3.667747\n",
      "Epoch: 1350 loss = 3.066088\n",
      "Epoch: 1360 loss = 4.274240\n",
      "Epoch: 1370 loss = 3.213422\n",
      "Epoch: 1380 loss = 2.359291\n",
      "Epoch: 1390 loss = 1.394334\n",
      "Epoch: 1400 loss = 1.057599\n",
      "Epoch: 1410 loss = 1.456951\n",
      "Epoch: 1420 loss = 1.643309\n",
      "Epoch: 1430 loss = 2.973422\n",
      "Epoch: 1440 loss = 1.747170\n",
      "Epoch: 1450 loss = 3.163108\n",
      "Epoch: 1460 loss = 2.259036\n",
      "Epoch: 1470 loss = 2.944038\n",
      "Epoch: 1480 loss = 0.888341\n",
      "Epoch: 1490 loss = 1.556689\n",
      "Epoch: 1500 loss = 0.617430\n",
      "Epoch: 1510 loss = 1.420827\n",
      "Epoch: 1520 loss = 1.024322\n",
      "Epoch: 1530 loss = 1.472634\n",
      "Epoch: 1540 loss = 1.197962\n",
      "Epoch: 1550 loss = 1.543951\n",
      "Epoch: 1560 loss = 1.917397\n",
      "Epoch: 1570 loss = 1.241019\n",
      "Epoch: 1580 loss = 0.699898\n",
      "Epoch: 1590 loss = 1.490545\n",
      "Epoch: 1600 loss = 2.788228\n",
      "Epoch: 1610 loss = 2.099351\n",
      "Epoch: 1620 loss = 1.502300\n",
      "Epoch: 1630 loss = 1.973302\n",
      "Epoch: 1640 loss = 2.429282\n",
      "Epoch: 1650 loss = 0.893386\n",
      "Epoch: 1660 loss = 0.428627\n",
      "Epoch: 1670 loss = 0.857633\n",
      "Epoch: 1680 loss = 0.923791\n",
      "Epoch: 1690 loss = 3.292355\n",
      "Epoch: 1700 loss = 2.293336\n",
      "Epoch: 1710 loss = 1.174155\n",
      "Epoch: 1720 loss = 0.973291\n",
      "Epoch: 1730 loss = 0.736930\n",
      "Epoch: 1740 loss = 0.886752\n",
      "Epoch: 1750 loss = 1.484077\n",
      "Epoch: 1760 loss = 1.103939\n",
      "Epoch: 1770 loss = 0.856244\n",
      "Epoch: 1780 loss = 0.564047\n",
      "Epoch: 1790 loss = 1.821950\n",
      "Epoch: 1800 loss = 0.936504\n",
      "Epoch: 1810 loss = 0.766704\n",
      "Epoch: 1820 loss = 0.691884\n",
      "Epoch: 1830 loss = 0.514938\n",
      "Epoch: 1840 loss = 0.919788\n",
      "Epoch: 1850 loss = 0.877344\n",
      "Epoch: 1860 loss = 0.679568\n",
      "Epoch: 1870 loss = 0.871412\n",
      "Epoch: 1880 loss = 0.984245\n",
      "Epoch: 1890 loss = 0.511915\n",
      "Epoch: 1900 loss = 0.277464\n",
      "Epoch: 1910 loss = 0.714353\n",
      "Epoch: 1920 loss = 0.535736\n",
      "Epoch: 1930 loss = 0.664504\n",
      "Epoch: 1940 loss = 0.711936\n",
      "Epoch: 1950 loss = 0.507977\n",
      "Epoch: 1960 loss = 1.281472\n",
      "Epoch: 1970 loss = 0.400548\n",
      "Epoch: 1980 loss = 1.037680\n",
      "Epoch: 1990 loss = 0.738983\n",
      "Epoch: 2000 loss = 0.466256\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "lr = 0.005\n",
    "batch_size = 16\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "log_path = \"log/log.txt\"\n",
    "model_path = \"model/model.pt\"\n",
    "model.load(model_path)\n",
    "with open(log_path, 'w', encoding='utf-8') as f:\n",
    "    for epoch in range(epochs):\n",
    "        batch = make_batch(data_set, batch_size, vocab).to(device)\n",
    "        out = model(batch)\n",
    "        loss = model.loss(out)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
    "            f.write('Epoch: {:04d} loss = {:.6f}\\n'.format(epoch + 1, loss))\n",
    "\n",
    "model.save(model_path)\n",
    "\n",
    "del model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 1.0008516311645508\n",
      "[1342, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "君耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆耆\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\Desktop\\File\\nlp-beginner\\task5\\modle.py:93: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(self.fc(output))\n"
     ]
    }
   ],
   "source": [
    "result_path = \"result/result.txt\"\n",
    "f = open(result_path, 'w', encoding='utf-8')\n",
    "\n",
    "model = LSTM_LM(vocab_size, dim_emb, hidden_size, num_layers, limit, device).to(device)\n",
    "model.load(model_path)\n",
    "\n",
    "test_batch = make_batch(data_set, 32, vocab).to(device)\n",
    "perplexity = model.calculate_perplexity(test_batch).item()\n",
    "print('perplexity:', perplexity)\n",
    "\n",
    "start = \"君\"\n",
    "start = vocab.word2id[start]\n",
    "\n",
    "output = model.generate(start)\n",
    "for i in output:\n",
    "    f.write(f\"{i}, \")\n",
    "print(output)\n",
    "output = vocab.ids2sentence(output)\n",
    "f.write(output)\n",
    "print(output)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
